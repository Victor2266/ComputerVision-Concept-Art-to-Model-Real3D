exp_name: 'FruitSV-consistency-rerender-lr4e-5-100-3'
exp_group: 'Training_on_Fruit'
output_dir: './output/'
log_dir: './log'
workers: 4
print_freq: 100
vis_freq: 250 #2000
eval_vis_freq: 10
seed: 42

dataset:
  # general settings
  img_size: 512
  num_frame: 4
  use_rand_crop: False
  img_crop_min: 128
  img_crop_max: 384
  white_bkg: True
  # multiview data setting (training)
  mv_data_name: 'objaverseWin2WSL'
  # single-view data setting (training)
  sv_data_use: True
  sv_data_name: 'wild-unfiltered-fruit-trainers'
  sv_render_views: 4
  sv_render_views_sample: 'constraint2_elevation'
  sv_curriculum: '15-90_15-90'  # initial-finial azimuth and elevation
  # single-view data setting (testing)
  sv_test_data_name: 'WildFruitTesters'


model:
  # use TripoSR
  pretrain_path: './checkpoint'
  model_name: 'model_both_trained_v1_for_TRAINING.ckpt'
  backbone_fix: False
  render_resolution: 128
  render_chunk_size: 49152 #32768 #24576 #16384
  render_num_samples_per_ray: 128


loss:
  weight_render_rgb: 2.0
  weight_render_mask: 1.0 #5.0 #5.0
  weight_perceptual: 1.0  # 2


train:
  resume: True
  pretrain_path: ''
  lr: 0.000004
  lr_embeddings: 0.00004
  lr_backbone: 0.00004
  beta1: 0.9  # 0.8
  beta2: 0.96
  eps: 0.000001
  weight_decay: 0.005   # 0.05 # Reduced since we're training fewer parameters
  warmup_iter: 200            # Reduced since we're training fewer parameters
  total_iteration: 10000
  grad_max: 0.5
  batch_size: 1  # Increased since we have memory headroom
  batch_size_sv: 1   # Increased since we have memory headroom
  accumulation_step: 4 # The main reason to keep accumulation_step > 1 would be to help smooth out gradient updates, Fruits share many common characteristics so less smooting is needed
  normalize_img: False # It centers the data around zero, It gives the data similar scale across all channels, These specific values are from ImageNet, # which the base model was trained on, Make training more stable, Speed up convergence # Help the model better utilize features learned from the pretrained weights
  use_amp: True
  # amp_dtype: "bfloat16"  # or "float16"
  use_checkpointing: True
  use_zeroRO: True
  use_consistency: True
  num_frame_consistency: 1
  rerender_consistency_input: True


test:
  batch_size: 1
  eval_resolution: 224
  eval_interval: 4
